{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab5 Clustering\n",
    "\n",
    "This week's exercise will focus on clustering using two common algorithms, k-means and k-medoids\n",
    "\n",
    "Schedule:\n",
    "* Implement the core of k-means\n",
    "* Clustering visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes matplotlib plots work better with Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "# Import the necessary libraries. \n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEFORE YOU START: Classification vs Clustering\n",
    "Most of this excercise, specially the first part, will be very similar to the Lab3 Classification. This is because classification and clustering are similar concepts, but with a very important difference:\n",
    "\n",
    "- Classification assigns labels from a pre-existing set\n",
    "- Clustering groups together data by similarity\n",
    "\n",
    "While k-nearest-neighbors returned somethin meaningful like 'GP' (GoalKeeper) and such, clustering algorithms will return something like 'Group1' or 'ClusterA'.\n",
    "While after classification the task of the data scientist is to understand if the classification is *correct*, after clustering they need to understand if the clusters are *meaningful*, and what meaning those could have.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at the data (again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check that data and data path is present\n",
    "basedir = \"./\"\n",
    "file = \"fifa.csv\"\n",
    "assert os.path.isdir(f\"{basedir}data\") and os.path.exists(f\"{basedir}data/{file}\"), 'Data not found. Make sure to have the most recent version!'\n",
    "\n",
    "data = pd.read_csv(f'{basedir}/data/fifa.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.FacetGrid(data, height=5,hue=\"Position\").map(plt.scatter,\"SprintSpeed\",\"Agility\").add_legend()\n",
    "sns.FacetGrid(data, height=5,hue=\"Position\", col='Preferred Foot').map(plt.scatter,\"ShotPower\",'Penalties').add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plots shows that the `Position` of a player could be related to some of their statistics. The dataset contains 30+ statistics and we don't know which ones will be most helpful, so we are picking an arbitrary subset to avoid the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality).\n",
    "It's also possible to choose the best ones [algorithmically](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#Feature_extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = ['Crossing','Finishing','HeadingAccuracy','ShortPassing','Volleys','Dribbling','Curve','FKAccuracy','LongPassing',\n",
    "            'BallControl','Acceleration','SprintSpeed','Agility','Reactions','Balance','ShotPower','Jumping','Stamina',\n",
    "            'Strength','LongShots','Aggression','Interceptions','Positioning','Vision','Penalties','Composure',\n",
    "            'Marking','StandingTackle','SlidingTackle','GKDiving','GKHandling','GKKicking','GKPositioning','GKReflexes']\n",
    "\n",
    "features = [\"Aggression\",\"Composure\",\"Penalties\",'SlidingTackle']\n",
    "# cleaning: remove all the lines that contain a NaN in one of the feature columns\n",
    "data = data.dropna(subset=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means\n",
    "\n",
    "0. pick a value for K (number of clusters)\n",
    "1. standardize fields (Reminder: subtract by mean, then divide by standard deviation)\n",
    "2. create K random centroids as arrays with size the number of features\n",
    "3. foreach `datapoint` in `data`:\n",
    "  1. set as `centroid` of `datapoint` the closest `centroid`\n",
    "  2. if at least one `centroid` has changed: goto 3\n",
    "  \n",
    "### Tips\n",
    "- the computations needed for k-means are expensive, test on a small subset of the data to save time (5/10 to check calculations on single rows, 50/100 to check the correctness of the whole algorithm)\n",
    "- you'll need to compute a distance at a certain point, check Lab3\n",
    "- [DataFrame.idxmin](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.idxmin.html]) will give you the index of the smallest entry in a dataset\n",
    "- `dataFrame1 is dataFrame2` is not correct (it checks if the two objects are the same, not if they contain the same values). The correct approach is `dataFrame1.equals(dataFrame2)` (or `dataFrame1['aColumn'].equals(dataFrame2['aColumn'])` ;))\n",
    "- You can apply a function to every element/row/column of a dataframe by using the `apply` pandas function\n",
    "- To update the values of the centroids, you might find the combination of `groupby` and `mean` to be useful.\n",
    "### Reminders:\n",
    "- Euclidian Distance:\n",
    "$$ d_e(a, b) = \\sqrt{\\sum_{i=1}^m{{(a_i-b_i)}^2}}$$\n",
    "- Minkowski Distance:\n",
    "$$ d_m(a, b) = \\sqrt[q]{\\sum_{i=1}^m{{\\lvert a_i-b_i \\rvert}^q}}$$\n",
    "- Chebyshev Distance:\n",
    "$$ d_c(a, b) = \\max(\\lvert a_1-b_1 \\rvert, \\lvert a_2-b_2 \\rvert, ..., \\lvert a_m-b_m \\rvert)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: define K\n",
    "K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Standardize features\n",
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_clustering(data, max_iter=-1):\n",
    "    \n",
    "    # Inizialize the centroids by sampling K elements from the data (do you remember the pandas sample function? :D)\n",
    "    # Then, remember to reset the indeces of the new centroids dataframe without introducing new columns\n",
    "    # (there's a pandas function for that as well :D)\n",
    "    ## YOUR CODE HERE\n",
    "    \n",
    "    # Implement the function assign_centroid that assigns the index of the closest centroid (Euclidian Distance)\n",
    "    def assign_centroid_euclidian(x):\n",
    "        ## YOUR CODE HERE\n",
    "        x['Centroid'] = #...\n",
    "        return x\n",
    "    \n",
    "    # Implement the function assign_centroid that assigns the index of the closest centroid (Minkowski Distance)\n",
    "    q = 8 # e.g.: q=1 -> Manhattan Distance, q=2 -> Euclidian Distance\n",
    "    def assign_centroid_minkowski(x):\n",
    "        ## YOUR CODE HERE\n",
    "        x['Centroid'] = #...\n",
    "        return x\n",
    "    \n",
    "    # Implement the function assign_centroid that assigns the index of the closest centroid (Chebyshev Distance)\n",
    "    def assign_centroid_chebyshev(x):\n",
    "        ## YOUR CODE HERE\n",
    "        x['Centroid'] = #...\n",
    "        return x\n",
    "\n",
    "    iter = 0\n",
    "    data = data.assign(Centroid=pd.Series([-1] * len(data)), OldCentroid=pd.Series([0] * len(data)))\n",
    "\n",
    "    # Implement the actual K-means loop (choose one of the 3 distance functions):\n",
    "    while not data['Centroid'].equals(data['OldCentroid']):\n",
    "        ## YOUR CODE HERE\n",
    "        \n",
    "    \n",
    "    print('done')\n",
    "    return data[features+['Centroid']], centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data, new_centroids = k_means_clustering(data.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusters visualization\n",
    "The most difficult task with clustering is understand if the clustering makes sense and what the clustering means. If the dimensionality is low, a good idea is to use a pairplot and see if the clusters are really close in each slice. The closer they are, the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# When plotting, choose a high contrast color pallette (possibly colorblind-friendly :) )\n",
    "sns.pairplot(new_data, vars=features, hue='Centroid', palette=sns.color_palette(\"colorblind\", K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
