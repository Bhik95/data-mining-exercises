{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J7IYFVoXJSd-"
   },
   "source": [
    "(Material adapted from Luis Fernando Laris Pardo's)\n",
    "# Autoencoders\n",
    "For today's exercise we are going to be implementing autoencoders. One cool implementation can be found [here](https://medium.com/@sorenlind/a-deep-convolutional-denoising-autoencoder-for-image-classification-26c777d3b88e) where autoencoders are used to get unnoisy images from magic the gathering cards. \n",
    "\n",
    "For this I have used the most common starter example for autoencoders that can be found using the mnist dataset. Remember that the autoencoder is doing something similar to the following:\n",
    "\n",
    "![image](https://www.pyimagesearch.com/wp-content/uploads/2020/02/keras_denoising_autoencoder_overview.png)\n",
    "\n",
    "As in previous weeks, most of the code is already given.\n",
    "\n",
    "**Your Task**:\n",
    "\n",
    "* Add artificial noise to the input images and see how your autoencoder works with these.\n",
    "* Add more dense layers to your autoencoder model\n",
    "\n",
    "**(Very) Optional Tasks:** Take a look at the articles at bottom page if you are going to work on this topic for your exam project.\n",
    "\n",
    "* Apply *[Image data augmentation](https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/)* to the dataset to improve the quality of the autoencoder\n",
    "* Create a *[CNN-based autoencoder](https://towardsdatascience.com/convolutional-autoencoders-for-image-noise-reduction-32fce9fc1763)*: Use convolutional, maxpooling and upscaling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nZJPE6FPzqz-",
    "outputId": "adc85b47-dd50-4432-d8ac-7169455f5bae"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Activation\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adadelta, SGD, Adam\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R_E-CPgCMKpx"
   },
   "source": [
    "### Read the data and flatten it\n",
    "In this case, first we are going to use dense layers, here is another way to flattern your data before giving it to the dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vd8A7BtSLvAB"
   },
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ItV_5A9iz1Me",
    "outputId": "a2f464b2-7412-47b5-9db5-5070cdd62769"
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# flatten the data (if working with Dense Network)\n",
    "\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:]))) \n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "# Add Noise to x_train, x_test\n",
    "x_train_noisy = x_train + np.random.normal(loc=0.0, scale=0.1, size=x_train.shape)\n",
    "x_test_noisy = x_test + np.random.normal(loc=0.0, scale=0.1, size=x_test.shape)\n",
    "\n",
    "# OPTIONAL: Data Augmentation - Transform the data (random translation/rotation/scale/shear/...)\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MSKi870MMdtv"
   },
   "source": [
    "### Create your autoencoder\n",
    "We can now create the model for the autoencoder, in this case is a very simple one where of an input layer, one hiden layer (the encoded part) and an output later (the decoded part)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ZzzEFXw0EIx"
   },
   "outputs": [],
   "source": [
    "# this is the size of our encoded representations\n",
    "encoding_dim = 32\n",
    "\n",
    "input_img = Input(shape=(784,))\n",
    "\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "# You can add multiple layers (of any type) in the following way, but remember to always create an \"hour-glass-shaped\" network:\n",
    "# encoded = Dense(<N_NEURONS>, activation=<ACTIVATION>)(encoded)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "saTWPiRNNB4g"
   },
   "source": [
    "### The encoder and decoder models\n",
    "This models we are going to use them later to test the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hv5B9yYa2k5-"
   },
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QQoEBrEkN0sn"
   },
   "source": [
    "### Time to train our model\n",
    "Try different optimizers, deffinitely SGD is not the best for this task. you should be able to ger good results with the setup given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UlQOqHXZ2oao"
   },
   "outputs": [],
   "source": [
    "opt = Adam()\n",
    "autoencoder.compile(optimizer=opt, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "8QXryo-v3HZP",
    "outputId": "ff88c4e3-eded-4ebc-8586-2e349ad43ea5"
   },
   "outputs": [],
   "source": [
    "# Use the noisy data as your input and the non-noisy data as the ground truth\n",
    "autoencoder.fit(x_train_noisy, x_train,\n",
    "                epochs=3, # CHANGE ME, PLIZ\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X9kwibqJOYI1"
   },
   "source": [
    "### Now let's test our Autoencoder!\n",
    "One important thing to notice is how reshape is used to rearrange the image plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xtAJO86Z3MSO"
   },
   "outputs": [],
   "source": [
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "id": "aVHmCaYB3PjB",
    "outputId": "dcf11702-f417-497b-a3b0-a70130ade4eb"
   },
   "outputs": [],
   "source": [
    "n = 10  \n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test_noisy[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gf6i5b0W8V2i"
   },
   "source": [
    "# Optional Articles\n",
    "\n",
    "### Image Data Augmentation\n",
    "https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/\n",
    "### Convolutional Autoencoders for Image Noise Reduction\n",
    "https://towardsdatascience.com/convolutional-autoencoders-for-image-noise-reduction-32fce9fc1763\n",
    "\n",
    "# Extra Articles\n",
    "\n",
    "### Anomaly Detection with Autoencoders\n",
    "https://towardsdatascience.com/anomaly-detection-with-autoencoder-b4cdce4866a6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOnfjuIE8V5X"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab09_Autoencoders.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
