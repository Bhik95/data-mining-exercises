{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association Mining!\n",
    "\n",
    "The problem - How do you determine which items belong together from data? \n",
    "For example imagine you are running a store and you want to know what items are frequently bought together - how would you go about this in an automated and systematic way?\n",
    "\n",
    "Apriori algorithm\n",
    "* pro: Easy to implement, easy to parallelize\n",
    "* con: Computationally expensive\n",
    "\n",
    "\n",
    "Market basket analysis\n",
    " * Better shop layout\n",
    " * Adverse effects of combining medicine\n",
    "\n",
    "Association rules are in the `if ... then ...` format. E.g:\n",
    "\n",
    "> **If** a customer buys tomatoes and carrots **then** they buy tofu.\n",
    "\n",
    "In formal logic this is called an implication, and is generally denoted with an arrow, so the statement above could be written:\n",
    "\n",
    "> `{tomatoes, carrots} -> tofu`\n",
    "\n",
    "Association mining in this case might lead us to the conclusion that 'tofu' should be considered as a vegetable, rather than as an oriental item, as it it is frequently bought by vegetarians.\n",
    "\n",
    "___\n",
    "\n",
    "Important metrics in apriori are:\n",
    "* Support - of an item set X is the proportion of observations in the dataset where X occurs\n",
    "    * $supp(X) = \\frac{\\text{# observations with X}}{\\text{Total # transactions}}$\n",
    "* Confidence - How strong is the rule? I.e. the confidence for `X->Y` is the likelihood that `Y` is purchased, if `X` is purchased. \n",
    "This is the same as the conditional probability.\n",
    "It is defined as follows:\n",
    "    * $conf(X \\rightarrow Y) = \\frac{supp(X \\cup Y)}{supp(X)}$\n",
    "* Lift\n",
    "    * If the support for `Y` is very high, then the confidence of `X->Y` might not be very informative. \n",
    "    Lift corrects for this.\n",
    "    * $lift(X\\rightarrow Y) = \\frac{supp(X \\cup Y)}{supp(X) supp(Y)}$\n",
    "\n",
    "\n",
    "Example work\n",
    "1. Load data\n",
    "1. Create itemsets of one-hot DataFrame\n",
    "1. Determine minimum support (and confidence) threshold\n",
    "    1. Some data exploration \n",
    "1. Find all items that satisfies this support threshold\n",
    "1. Find all combinations of the items that satisfy the support threshold. You can use arbitrarily large sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes matplotlib plots work better with Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "# Import the necessary libraries. \n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json # <- this package is super useful to read/write json data :D\n",
    "from pprint import pprint\n",
    "from itertools import combinations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data\n",
    "\n",
    "For this exercise we will be working with classical shopping-cart data.\n",
    "The cell below loads the data and prints the first example.\n",
    "- After loading a json file, you can access its elements with \"\\[\\]\" just like you would do with a python array or dictionary:\n",
    "  - data_json[0] -> Get the first entry\n",
    "  - data_json[0][\"items\"] -> Get the list of items of the first entry "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check that data and data path is present\n",
    "data_folder = \"./data/\"\n",
    "data_filename = \"shopping.json\"\n",
    "assert os.path.isdir(data_folder) and os.path.exists(data_folder + data_filename), 'Data not found. Make sure to have the most recent version!'\n",
    "\n",
    "# Load json data\n",
    "with open(data_folder + data_filename) as f:\n",
    "    data_json = json.load(f)\n",
    "\n",
    "# Examine data\n",
    "print('Number of observations', len(data_json))\n",
    "pprint(data_json[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Transforming the Data\n",
    "\n",
    "The first thing we want to do is transform the data into a one-hot (numpy) matrix.\n",
    "A one-hot matrix is a matrix where each column represent an item, e.g. `ice cream`, and each row represents a transaction.\n",
    "`0` indicates the item wasn't bought, and `1` indicates that it was.\n",
    "\n",
    "Your task is to\n",
    "1. Create a list of all the feature names, but remove the uninformative ones, i.e. the ones in `bad_feature_names`\n",
    "1. Create a one-hot numpy array of the data, and name it `data`\n",
    " * Note: If an item occurs twice in a transaction we still just want to have a `1` in the one-hot matrix.\n",
    " * Hint: The array should have shape `(1499, 37)`\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_observations = len(data_json)\n",
    "bad_feature_names = ['', 'all- purpose'] \n",
    "\n",
    "# Make a set with all of the possible items\n",
    "all_items = set()\n",
    "## YOUR CODE HERE\n",
    "\n",
    "# Remove bad item names from all_items\n",
    "## YOUR CODE HERE\n",
    "        \n",
    "# Convert all_items to a list to get the feature names (from now on the order is important)\n",
    "features_names = ## YOUR CODE HERE\n",
    "\n",
    "\n",
    "print('feature_names')\n",
    "print(features_names)\n",
    "print()\n",
    "\n",
    "# Create a one-hot matrix (data)\n",
    "num_features = len(features_names)\n",
    "data = np.zeros([num_observations, num_features])\n",
    "## YOUR CODE HERE\n",
    "\n",
    "print('The data looks like this now:')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Support\n",
    "\n",
    "Now we want to compute the support of each of the individual items, and visualize it.\n",
    "\n",
    "1. The `compute_support` function computes the support for an itemset. \n",
    " * E.g. `[0,1,2]` would be `{'aluminum foil', 'bagels', 'beef',}`, which occurs 7.20% of the time.\n",
    "1. Compute the support of each of the items individually - i.e. how frequent are each item.\n",
    "1. Visualize the support as a histogram.\n",
    "    * `plt.xticks` can be used to rotate the labels on the x-axis for a prettier plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_support(matrix, itemset):\n",
    "    \"\"\" Given a one-hot array matrix, and an itemset (a list), indicating the columns of interest\n",
    "        compute_support returns the support of the elements in itemset.\n",
    "        \n",
    "        The support of an item set X is the proportion of observations in the dataset where X occurs.\n",
    "        $supp(X) = \\frac{\\text{# observations with X}}{\\text{Total # transactions}}$\n",
    "    \"\"\"\n",
    "    rule = matrix[:, itemset].all(axis=1)\n",
    "    support = rule.sum() / matrix.shape[0]\n",
    "    return support\n",
    "\n",
    "print('Support for [0,1,2]:', compute_support(data, [0,1,2]))\n",
    "\n",
    "# Calculate the support for each 1-Itemset (1-Itemset = itemset composed by one single item)\n",
    "support = []\n",
    "for i in range(data.shape[1]):\n",
    "    support.append(compute_support(data, (i,)))\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "g = sns.barplot(x=features_names, y=support, color='b')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Creating M+1-Itemsets\n",
    "\n",
    "Here we want to implement and test a helper function `generate_M_plus_1_itemsets`.\n",
    "The function takes a `NxM` **list** of item sets.\n",
    "`N` indicates the number of item sets, and `M` indicates the length.\n",
    "The function should then produce a new **list** with all the new itemsets of length `M+1`.\n",
    "\n",
    "Your task:\n",
    "1. Fill out the function `generate_M_plus_1_itemsets`. The output must be a **list** of itemsets. Each itemset is a **list**.\n",
    " * Note: Watch out you **don't create duplicates!** We don't care about the order, so `[0, 1]` is the same as `[1, 0]`.\n",
    " * It is a good idea to start by finding all the unique values in old_combinations.\n",
    "1. Run the test in the next cell, and make sure that you get the correct result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_M_plus_1_itemsets(old_combinations):\n",
    "    \"\"\" Input: The NxM list of M-itemsets, \n",
    "            N: number of combinations\n",
    "            M: length of each item set\n",
    "        Output: The ?xM+1 list of M+1-itemsets\n",
    "    \"\"\"\n",
    "    \n",
    "    items_types_in_previous_step = np.unique(old_combinations)\n",
    "    new_combinations = []\n",
    "    ## YOUR CODE HERE \n",
    "    \n",
    "    return new_combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below serves as a test of `generate_M_plus_1_itemsets`. \n",
    "If your function is written correctly the output shold be as follows:\n",
    "\n",
    "    Itemset length: 1\n",
    "    [[0], [1], [2], [3]]\n",
    "\n",
    "    Itemset length: 2\n",
    "    [[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3]]\n",
    "\n",
    "    Itemset length: 3\n",
    "    [[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]]\n",
    "\n",
    "    Itemset length: 4\n",
    "    [[0, 1, 2, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of candidate_generation\n",
    "comb = [[i] for i in range(0, 4)]\n",
    "while len(comb)>0:\n",
    "    print('Itemset length:', len(comb[0]))\n",
    "    print(comb)\n",
    "    print()\n",
    "    comb = generate_M_plus_1_itemsets(comb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Apriori Step 1: Find Frequent Itemsets\n",
    "\n",
    "In this task we will find all the sets that have sufficient support.\n",
    "\n",
    "Your task:\n",
    "1. Finish the code below using what you have already done in the previous tasks. Each iteration of the loop should:\n",
    " * Generate all relevant combinations.\n",
    " * For each combination compute the support\n",
    " * If the support is larger than `min_support` save the item set and the support in `itemset_dict` and `support_dict`.\n",
    "1. Convert your results, `itemset_dict` and `support_dict`, into a `pandas` dataframe.\n",
    " * Use `reset_index` to fix the indexes (remove the unwanted 'history')\n",
    " * Convert the item sets to `frozenset`. `frozenset` is a [immutable data structure for sets](https://www.programiz.com/python-programming/methods/built-in/frozenset) that enables common set operations like union, intersection, difference (some of which may come in handy later *wink wink*)\n",
    "1. Find an appropriate minimum support (subjective). Run the code several times, and see how changing this value changes the outcome.\n",
    " * You may choose a maximum length as well if you want to.\n",
    "1. Print the results using the actual feature names - how does it look? Any surprises?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_support = 0.3\n",
    "\n",
    "# Find the frequent 1-itemsets by calculating their support\n",
    "# Store the supports and the frequent 1-itemsets in 2 dictionaries\n",
    "# (frequent_1_itemsets_support and frequent_1_itemsets) at key=1\n",
    "frequent_1_itemsets_support = []\n",
    "frequent_1_itemsets = []\n",
    "for i in range(data.shape[1]):\n",
    "    support = compute_support(data, [i])\n",
    "    if support >= min_support:\n",
    "        frequent_1_itemsets_support.append(support)\n",
    "        frequent_1_itemsets.append([i])\n",
    "support_dict = {1: frequent_1_itemsets_support}\n",
    "itemset_dict = {1: frequent_1_itemsets}\n",
    "\n",
    "rows_count = float(data.shape[0])\n",
    "itemset_len = 1\n",
    "while itemset_len > 0:\n",
    "\n",
    "    next_itemset_len = itemset_len + 1\n",
    "    \n",
    "    frequent_itemsets = []\n",
    "    frequent_itemsets_support = []\n",
    "\n",
    "    # Generate the k+1 itemsets\n",
    "    new_itemsets = ## YOUR CODE HERE\n",
    "    # Store the frequent itemsets and their support in frequent_itemsets and frequent_itemsets_support respectively\n",
    "    for new_itemset in new_itemsets:\n",
    "        ## YOUR CODE HERE\n",
    "\n",
    "    if len(frequent_itemsets):\n",
    "        # Add the frequent itemsets and their support to the dictionaries at key=next_itemset_len\n",
    "        ## YOUR CODE HERE\n",
    "\n",
    "        itemset_len = next_itemset_len\n",
    "    else:\n",
    "        itemset_len = 0\n",
    "\n",
    "# 2 Convert the 2 dictionaries into a Pandas dataframe\n",
    "all_res = []\n",
    "for k in sorted(itemset_dict):\n",
    "    support = pd.Series(support_dict[k])\n",
    "    itemsets = pd.Series([frozenset(i) for i in itemset_dict[k]]) # <- notice the frozenset (see info above)\n",
    "\n",
    "    res = pd.concat((support, itemsets), axis=1)\n",
    "    all_res.append(res)\n",
    "\n",
    "res_df = pd.concat(all_res)\n",
    "res_df.columns = ['support', 'itemsets']\n",
    "res_df = res_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Print results in a human readable manner\n",
    "for i in range(37, len(res_df)):\n",
    "    print([features_names[i] for i in res_df.itemsets[i]], '\\t has support {:.4f}'.format(res_df.support[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Confidence, Lift\n",
    "\n",
    "Complete the functions computing confidence, and lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confidence(supportAntecedentConsequent, supportAntecedent):\n",
    "    \"\"\" The confidence for `X->Y` is the likelihood that `Y` is purchased, if `X` is purchased. \n",
    "        This is the same as the conditional probability.\n",
    "        $conf(X \\rightarrow Y) = \\frac{supp(X \\cup Y)}{supp(X)}$\n",
    "    \"\"\"\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "\n",
    "def compute_lift(supportAntecedentConsequent, supportAntecedent, supportConsequent):\n",
    "    \"\"\" \n",
    "        $lift(X\\rightarrow Y) = \\frac{supp(X \\cup Y)}{supp(X) supp(Y)}$        \n",
    "    \"\"\"\n",
    "    ## YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apriori Step 2: Find the Association Rules\n",
    "\n",
    "Now we put it all together, and complete the apriori algorithm!\n",
    "\n",
    "1. Write a for-loop that for each set of size 2 or larger\n",
    " * computes all possilbe association rules (e.g. {A, B, C} becomes {{A -> B, C}, {A, B -> C}, {A, C -> B}, {B, C -> A})\n",
    " * for each association rule compute the score (`score_function`). \n",
    " * If the score is larger than the threshold save the support for the entire set, the antecedent, and the concequent.\n",
    "1. Create a pandas dataframe with all the metrics, and have a look at it.\n",
    "1. Experiment with different selection metrics and thresholds. What gives results that look reasonable?\n",
    "\n",
    "You can generate all the combinations of size *size* with [Itertools.combinations(A, size)](https://www.geeksforgeeks.org/python-itertools-combinations-function/#:~:text=Similarly%20itertools.,are%20emitted%20in%20lexicographical%20order.). For example, if `A=['a','b','c']`, then `list(combinations(A, 2))` is `[['a', 'b'], ['a', 'c'], ['b', 'c']]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_function = lambda supAC, supA, supC: compute_confidence(supAC, supA)\n",
    "min_threshold = 0.3\n",
    "\n",
    "itemsets = res_df['itemsets'].values\n",
    "itemsets_support = res_df['support'].values\n",
    "\n",
    "frequent_items_dict = {}\n",
    "for i in range(len(itemsets)):\n",
    "    frequent_items_dict[frozenset(itemsets[i])] = itemsets_support[i]\n",
    "\n",
    "# prepare buckets to collect frequent rules\n",
    "rule_antecedents = []\n",
    "rule_consequents = []\n",
    "rule_supports = []\n",
    "\n",
    "# iterate over all frequent itemsets\n",
    "for itemset in frequent_items_dict.keys():\n",
    "    supportAntCons = frequent_items_dict[itemset]\n",
    "\n",
    "    # find all possible combinations of itemsets\n",
    "    for idx in range(len(itemset)-1, 0, -1):\n",
    "        # of antecedent and consequent\n",
    "        for subset in combinations(itemset, r=idx): # loop over the combinations of size idx (see combinations from package itertools)\n",
    "            antecedent = frozenset(subset)\n",
    "            consequent = itemset.difference(antecedent)\n",
    "\n",
    "            supportAnt = frequent_items_dict[antecedent]\n",
    "            supportCons = frequent_items_dict[consequent]\n",
    "            score = score_function(supportAntCons, supportAnt, supportCons)\n",
    "            if score >= min_threshold:\n",
    "                rule_antecedents.append(antecedent)\n",
    "                rule_consequents.append(consequent)\n",
    "                rule_supports.append([supportAntCons, supportAnt, supportCons])\n",
    "\n",
    "# generate metrics\n",
    "rule_supports = np.array(rule_supports).T.astype(float)\n",
    "\n",
    "# convert ids to item string name\n",
    "rule_antecedents_string = [frozenset(features_names[j] for j in i) for i in rule_antecedents]\n",
    "rule_consequents_string = [frozenset(features_names[j] for j in i) for i in rule_consequents]\n",
    "\n",
    "df_res = pd.DataFrame(\n",
    "    data=list(zip(rule_antecedents_string, rule_consequents_string)),\n",
    "    columns=[\"antecedents\", \"consequents\"])\n",
    "\n",
    "supportAntCons = rule_supports[0]\n",
    "supportAnt = rule_supports[1]\n",
    "supportCons = rule_supports[2]\n",
    "\n",
    "df_res['total_support'] = supportAntCons\n",
    "df_res['antecedent_support'] = supportAnt\n",
    "df_res['consequent_support'] = supportCons\n",
    "df_res['confidence'] = compute_confidence(supportAntCons, supportAnt)\n",
    "df_res['lift'] = compute_lift(supportAntCons, supportAnt, supportCons)\n",
    "\n",
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
